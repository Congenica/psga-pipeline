env {
    PSGA_ROOT_PATH = "/app"

    // docker image config
    DOCKER_IMAGE_PREFIX = "${DOCKER_IMAGE_PREFIX}"

    // aws config
    AWS_CONNECTION_TIMEOUT = "${AWS_CONNECTION_TIMEOUT}"
    AWS_MAX_CONNECTIONS = "${AWS_MAX_CONNECTIONS}"
    AWS_MAX_PARALLEL_TRANSFERS = "${AWS_MAX_PARALLEL_TRANSFERS}"


    // k8s config
    K8S_NODE = "${K8S_NODE}"
    K8S_PULL_POLICY = "${K8S_PULL_POLICY}"
    K8S_SERVICE_ACCOUNT = "${K8S_SERVICE_ACCOUNT}"
    K8S_QUEUE_SIZE = "${K8S_QUEUE_SIZE}"
    K8S_STORAGE_CLAIM_NAME = "${K8S_STORAGE_CLAIM_NAME}"
    K8S_STORAGE_MOUNT_PATH = "${K8S_STORAGE_MOUNT_PATH}"
    K8S_PROCESS_MAX_RETRIES = "${K8S_PROCESS_MAX_RETRIES}"
    K8S_PROCESS_CPU_LOW = "${K8S_PROCESS_CPU_LOW}"
    K8S_PROCESS_CPU_HIGH = "${K8S_PROCESS_CPU_HIGH}"
    K8S_PROCESS_MEMORY_VERY_LOW = "${K8S_PROCESS_MEMORY_VERY_LOW}"
    K8S_PROCESS_MEMORY_LOW = "${K8S_PROCESS_MEMORY_LOW}"
    K8S_PROCESS_MEMORY_MEDIUM = "${K8S_PROCESS_MEMORY_MEDIUM}"
    K8S_PROCESS_MEMORY_HIGH = "${K8S_PROCESS_MEMORY_HIGH}"
    K8S_PROCESS_MEMORY_VERY_HIGH = "${K8S_PROCESS_MEMORY_VERY_HIGH}"

    // nextflow internal env vars
    NXF_WORK = "${NXF_WORK}"
    NXF_EXECUTOR = "${NXF_EXECUTOR}"
    NXF_ANSI_LOG = "false"
    NXF_OPTS = "${NXF_OPTS}"
}

params {
    // analysis run name
    run = ""
    // path to the metadata CSV file
    metadata = ""
    // e.g. 'illumina', 'ont', 'unknown'
    sequencing_technology = ""
    // the kit used for sequencing the sample (e.g. 'V4' as primer-scheme version)
    kit = ""
    // the path to the pipeline output. This can be s3
    output_path = ""

    /* Optional parameters */
    help = false
    print_config = false
}

k8s {
   // use k8s jobs instead of pods. See: https://github.com/nextflow-io/nextflow/pull/2751
   computeResourceType = 'Job'

   pullPolicy = "${env.K8S_PULL_POLICY}"
   serviceAccount = "${env.K8S_SERVICE_ACCOUNT}"
   storageClaimName = "${env.K8S_STORAGE_CLAIM_NAME}"
   storageMountPath = "${env.K8S_STORAGE_MOUNT_PATH}"
   storageSubPath = ''
}

aws {
    client {
        maxConnections = "${env.AWS_MAX_CONNECTIONS}"
        connectionTimeout = "${env.AWS_CONNECTION_TIMEOUT}"
        maxErrorRetry = 5
    }
    batch {
        maxParallelTransfers = "${env.AWS_MAX_PARALLEL_TRANSFERS}"
        maxTransferAttempts = 5
    }
}

executor {
    queueSize = "${env.K8S_QUEUE_SIZE}"
    submitRateLimit = '10 sec'
}

plugins {
  // used for eks
  id 'nf-amazon@1.10.0'
}

process {

    // default directives. These can be fine-tuned per process
    cpus = "${env.K8S_PROCESS_CPU_LOW}"
    memory = { "${env.K8S_PROCESS_MEMORY_LOW}" as int * 1.MB * task.attempt }
    maxRetries = "${env.K8S_PROCESS_MAX_RETRIES}"
    // by default, re-run a process if exit status is not 0 for max retries.
    // if the process still fails, terminate the pipeline
    errorStrategy = { (task.exitStatus != 0 && task.attempt <= "${env.K8S_PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'terminate' }

    // The scratch directive allows you to execute the process in a temporary folder that is local to the execution node.
    // This is useful when your pipeline is launched by using a grid executor, because it allows you to decrease the NFS
    // overhead by running the pipeline processes in a temporary directory in the local disk of the actual execution node.
    // Only the files declared as output in the process definition will be copied in the pipeline working area.
    scratch = 'true'

    // run the pod in the specified node affinity
    // Note the extra array brackets because matchExpressions is inside an array element of nodeSelectorTerms.
    pod = [
      affinity: [
        nodeAffinity: [
          requiredDuringSchedulingIgnoredDuringExecution: [
            nodeSelectorTerms: [[
              matchExpressions: [
                [key: "${env.K8S_NODE}", operator: "In", values: ["true"]]
              ]
            ]]
          ]
        ]
      ]
    ]
}

// NOTE: these are not uploaded to s3. Looks like this is a bug in Nextflow as I've seen documentation for this use case

// enable these passing flags like: -with-trace
trace {
  enabled = false
  file = "${params.run}_trace.csv"
  sep = ','
  raw = true  // time: ms; memory: bytes
  overwrite = true
  //default:
  //fields = 'task_id,hash,native_id,name,status,exit,submit,duration,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar'
  fields = 'task_id,hash,native_id,name,status,exit,submit,start,complete,duration,memory,attempt,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar,error_action'
}

//send trace scope information as HTTP POST request to a webserver, shipped as a JSON object
weblog {
  enabled = false
  url = 'http://localhost'
}

report {
  enabled = false
  file = "${params.run}_report.html"
  overwrite = true
}

timeline {
  enabled = false
  file = "${params.run}_timeline.html"
  overwrite = true
}

dag {
  enabled = false
  file = "${params.run}_dag.svg"
  overwrite = true
}
