manifest {
    name = 'Congenica PSGA pipeline'
    description = 'Pathogen Sequence Genome Analysis pipeline'
    author = 'Congenica'
    homePage = 'https://www.congenica.com/'
    mainScript = 'main.nf'
    nextflowVersion = '>= 21.10.4'
    version = '1.0.0'
}

env {
    PSGA_ROOT_PATH = "${PSGA_ROOT_PATH}"
    PSGA_OUTPUT_PATH = "${PSGA_OUTPUT_PATH}"
    PSGA_INCOMPLETE_ANALYSIS_RUNS_PATH = "${PSGA_INCOMPLETE_ANALYSIS_RUNS_PATH}"

    // docker image config
    DOCKER_IMAGE_PREFIX = "${DOCKER_IMAGE_PREFIX}"
    PSGA_DOCKER_IMAGE_TAG = "${PSGA_DOCKER_IMAGE_TAG}"

    // k8s config
    K8S_PULL_POLICY = "${K8S_PULL_POLICY}"
    K8S_SERVICE_ACCOUNT = "${K8S_SERVICE_ACCOUNT}"
    K8S_QUEUE_SIZE = "${K8S_QUEUE_SIZE}"
    K8S_STORAGE_CLAIM_NAME = "${K8S_STORAGE_CLAIM_NAME}"
    K8S_STORAGE_MOUNT_PATH = "${K8S_STORAGE_MOUNT_PATH}"
    K8S_PROCESS_MAX_RETRIES = "${K8S_PROCESS_MAX_RETRIES}"
    K8S_PROCESS_CPU_LOW = "${K8S_PROCESS_CPU_LOW}"
    K8S_PROCESS_CPU_HIGH = "${K8S_PROCESS_CPU_HIGH}"
    K8S_PROCESS_MEMORY_VERY_LOW = "${K8S_PROCESS_MEMORY_VERY_LOW}"
    K8S_PROCESS_MEMORY_LOW = "${K8S_PROCESS_MEMORY_LOW}"
    K8S_PROCESS_MEMORY_MEDIUM = "${K8S_PROCESS_MEMORY_MEDIUM}"
    K8S_PROCESS_MEMORY_HIGH = "${K8S_PROCESS_MEMORY_HIGH}"
    K8S_PROCESS_MEMORY_VERY_HIGH = "${K8S_PROCESS_MEMORY_VERY_HIGH}"

    // nextflow internal env vars
    NXF_WORK = "${NXF_WORK}"
    NXF_EXECUTOR = "${NXF_EXECUTOR}"
    NXF_ANSI_LOG = "${NXF_ANSI_LOG}"
    NXF_OPTS = "${NXF_OPTS}"
}

params {
    // directory containing the nextflow configs and workflows for a certain pathogen.
    // Override this in the pathogen-specific config, not here
    pathogen_dir = ""

    // analysis run name
    run = ""

    // path to the metadata CSV file
    metadata = ""

    /* Optional parameters */
    help = false
    print_config = false
}

executor {
    name = 'k8s'
    queueSize = "${env.K8S_QUEUE_SIZE}"
    submitRateLimit = '10 sec'
}

k8s {
   // use k8s jobs instead of pods. See: https://github.com/nextflow-io/nextflow/pull/2751
   computeResourceType = 'Job'

   pullPolicy = "${env.K8S_PULL_POLICY}"
   serviceAccount = "${env.K8S_SERVICE_ACCOUNT}"
   storageClaimName = "${env.K8S_STORAGE_CLAIM_NAME}"
   storageMountPath = "${env.K8S_STORAGE_MOUNT_PATH}"
   storageSubPath = ''
}

process {

    // default directives. These can be fine-tuned per process
    // use the local executor by default, so that the filtering-processes (which are only used for organising files) and any
    // dummy processes (e.g. pipeline complete) are executed in the same container where the main pipeline runs.
    executor = 'local'
    cpus = "${env.K8S_PROCESS_CPU_LOW}"
    memory = { "${env.K8S_PROCESS_MEMORY_LOW}" as int * 1.MB * task.attempt }
    maxRetries = "${env.K8S_PROCESS_MAX_RETRIES}"
    errorStrategy = { (task.exitStatus in 137..143 && task.attempt <= "${env.K8S_PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'terminate' }

}

// NOTE: these are not uploaded to s3. Looks like this is a bug in Nextflow as I've seen documentation for this use case

// enable these passing flags like: -with-trace
trace {
  enabled = false
  file = "${params.run}_trace.csv"
  sep = ','
  raw = true  // time: ms; memory: bytes
  overwrite = true
  //default:
  //fields = 'task_id,hash,native_id,name,status,exit,submit,duration,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar'
  fields = 'task_id,hash,native_id,name,status,exit,submit,start,complete,duration,memory,attempt,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar,error_action'
}

//send trace scope information as HTTP POST request to a webserver, shipped as a JSON object
weblog {
  enabled = false
  url = 'http://localhost'
}

report {
  enabled = false
  file = "${params.run}_report.html"
  overwrite = true
}

timeline {
  enabled = false
  file = "${params.run}_timeline.html"
  overwrite = true
}

dag {
  enabled = false
  file = "${params.run}_dag.svg"
  overwrite = true
}
