includeConfig './common/common.config'

env {
    PSGA_ROOT_PATH = "/app"
    // store temporary files in the scratch directory instead of nextflow usual work directory
    // use false only for testing
    SCRATCH = "${SCRATCH?:'true'}"

    // docker image config
    DOCKER_IMAGE_URI_PATH = "${DOCKER_IMAGE_URI_PATH}"
    DOCKER_IMAGE_TAG = "${DOCKER_IMAGE_TAG}"

    // aws config
    AWS_CONNECTION_TIMEOUT = "${AWS_CONNECTION_TIMEOUT?:'1000000'}"
    AWS_MAX_CONNECTIONS = "${AWS_MAX_CONNECTIONS?:'20'}"
    AWS_MAX_PARALLEL_TRANSFERS = "${AWS_MAX_PARALLEL_TRANSFERS?:'10'}"

    // aws batch config
    QUEUE = "${QUEUE}"

    // k8s config
    K8S_NODE = "${K8S_NODE}"
    K8S_PULL_POLICY = "${K8S_PULL_POLICY?:'Always'}"
    K8S_SERVICE_ACCOUNT = "${K8S_SERVICE_ACCOUNT}"
    K8S_STORAGE_CLAIM_NAME = "${K8S_STORAGE_CLAIM_NAME}"
    K8S_STORAGE_MOUNT_PATH = "${K8S_STORAGE_MOUNT_PATH}"

    // Process
    QUEUE_SIZE = "${QUEUE_SIZE}"
    PROCESS_MAX_RETRIES = "${PROCESS_MAX_RETRIES?:'3'}"
    PROCESS_CPU_LOW = "${PROCESS_CPU_LOW?:'1'}"
    PROCESS_CPU_HIGH = "${PROCESS_CPU_HIGH?:'2'}"
    PROCESS_MEMORY_VERY_LOW = "${PROCESS_MEMORY_VERY_LOW?:'8000'}"
    PROCESS_MEMORY_LOW = "${PROCESS_MEMORY_LOW?:'8000'}"
    PROCESS_MEMORY_MEDIUM = "${PROCESS_MEMORY_MEDIUM?:'8000'}"
    PROCESS_MEMORY_HIGH = "${PROCESS_MEMORY_HIGH?:'8000'}"
    PROCESS_MEMORY_VERY_HIGH = "${PROCESS_MEMORY_VERY_HIGH?:'16000'}"

    SARS_COV_2_PIPELINE_DOCKER_IMAGE = "${SARS_COV_2_PIPELINE_DOCKER_IMAGE?:'sars-cov-2-pipeline'}"
    NCOV_DOCKER_IMAGE_NAME = "ncov2019-artic-nf-${params.sequencing_technology == "ont" ? "nanopore" : params.sequencing_technology}"

    HOST_MOUNT_POINT = "${HOST_MOUNT_POINT?:'/app/resources'}"
    RESOURCE_MOUNT_POINT = "${RESOURCE_MOUNT_POINT?:'/app/resources'}"

}

params {
    // TODO: Handle this MUCH better
    contamination_removal_empty_csv = "/app/scripts/contamination_removal_empty.csv"
    primer_autodetection_empty_csv = "/app/scripts/primer_autodetection_empty.csv"
    ncov_qc_empty_csv = "/app/scripts/ncov_qc_empty.csv"
    ncov_typing_empty_csv = "/app/scripts/ncov_typing_empty.csv"
    pangolin_empty_csv = "/app/scripts/pangolin_empty.csv"
}

process {
    cpus = "${env.PROCESS_CPU_LOW}"
    memory = { "${env.PROCESS_MEMORY_LOW}" as int * 1.MB * task.attempt }
    maxRetries = "${env.PROCESS_MAX_RETRIES}"
    // by default, re-run a process if exit status is not 0 for max retries.
    // if the process still fails, terminate the pipeline
    errorStrategy = { (task.exitStatus != 0 && task.attempt<= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'terminate' }

    container = "${env.DOCKER_IMAGE_URI_PATH}/${env.SARS_COV_2_PIPELINE_DOCKER_IMAGE}:${env.DOCKER_IMAGE_TAG}"
    queue = "${env.QUEUE}"

    // The scratch directive allows you to execute the process in a temporary folder that is local to the execution node.
    // This is useful when your pipeline is launched by using a grid executor, because it allows you to decrease the NFS
    // overhead by running the pipeline processes in a temporary directory in the local disk of the actual execution node.
    // Only the files declared as output in the process definition will be copied in the pipeline working area.
    scratch = "${env.SCRATCH}"

    pod = [
      [hostPath: "${env.HOST_MOUNT_POINT}", "mountPath": "${env.RESOURCE_MOUNT_POINT}"],
      [
        affinity: [
          nodeAffinity: [
            requiredDuringSchedulingIgnoredDuringExecution: [
              nodeSelectorTerms: [[
                matchExpressions: [
                  [key: "${env.K8S_NODE}", operator: "In", values: ["true"]]
                ]
              ]]
            ]
          ]
        ]
      ]
    ]

    withName:NCOV2019_ARTIC_NF_PIPELINE {
        container = "${env.DOCKER_IMAGE_URI_PATH}/${env.NCOV_DOCKER_IMAGE_NAME}:${env.DOCKER_IMAGE_TAG}"
        memory = { "${env.PROCESS_MEMORY_VERY_HIGH}" as int * 1.MB * task.attempt }
        cpus = "${env.PROCESS_CPU_HIGH}" // TODO: only for illumina
        // if the computation for 1 sample dies, do not interrupt the computation for the other samples
        errorStrategy = { (task.exitStatus != 0 && task.attempt<= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'ignore' }
    }

    withName:PANGOLIN_PIPELINE {
        container = "${env.DOCKER_IMAGE_URI_PATH}/pangolin:${env.DOCKER_IMAGE_TAG}"
        memory = { "${env.PROCESS_MEMORY_HIGH}" as int * 1.MB * task.attempt }
        // if the computation for 1 sample dies, do not interrupt the computation for the other samples
        errorStrategy = { (task.exitStatus != 0 && task.attempt    <= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'ignore' }
    }

    withName:REHEADER_FASTA {
        memory = { "${env.PROCESS_MEMORY_MEDIUM}" as int * 1.MB * task.attempt }
        // if the computation for 1 sample dies, do not interrupt the computation for the other samples
        errorStrategy = { (task.exitStatus != 0 && task.attempt    <= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'ignore' }
    }
}

aws {
    client {
        maxConnections = "${env.AWS_MAX_CONNECTIONS}"
        connectionTimeout = "${env.AWS_CONNECTION_TIMEOUT}"
        maxErrorRetry = 5
    }
    batch {
        maxParallelTransfers = "${env.AWS_MAX_PARALLEL_TRANSFERS}"
        maxTransferAttempts = 5
        volumes = ["${env.HOST_MOUNT_POINT}:${env.RESOURCE_MOUNT_POINT}:ro"]
    }
}

k8s {
   // use k8s jobs instead of pods. See: https://github.com/nextflow-io/nextflow/pull/2751
   computeResourceType = 'Job'

   pullPolicy = "${env.K8S_PULL_POLICY}"
   serviceAccount = "${env.K8S_SERVICE_ACCOUNT}"
   storageClaimName = "${env.K8S_STORAGE_CLAIM_NAME}"
   storageMountPath = "${env.K8S_STORAGE_MOUNT_PATH}"
   storageSubPath = ''
}

executor {
    queueSize = "${env.QUEUE_SIZE}"
}


// NOTE: these are not uploaded to s3. Looks like this is a bug in Nextflow as I've seen documentation for this use case

// enable these passing flags like: -with-trace
trace {
  enabled = false
  file = "${params.run}_trace.csv"
  sep = ','
  raw = true  // time: ms; memory: bytes
  overwrite = true
  //default:
  //fields = 'task_id,hash,native_id,name,status,exit,submit,duration,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar'
  fields = 'task_id,hash,native_id,name,status,exit,submit,start,complete,duration,memory,attempt,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar,error_action'
}

//send trace scope information as HTTP POST request to a webserver, shipped as a JSON object
weblog {
  enabled = false
  url = 'http://localhost'
}

report {
  enabled = false
  file = "${params.run}_report.html"
  overwrite = true
}

timeline {
  enabled = false
  file = "${params.run}_timeline.html"
  overwrite = true
}

dag {
  enabled = false
  file = "${params.run}_dag.svg"
  overwrite = true
}