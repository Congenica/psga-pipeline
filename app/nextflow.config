env {
    PSGA_ROOT_PATH = "/app"

    // docker image config
    DOCKER_IMAGE_URI_PATH = "${DOCKER_IMAGE_URI_PATH}"
    DOCKER_IMAGE_TAG = "${DOCKER_IMAGE_TAG}"

    // aws config
    AWS_CONNECTION_TIMEOUT = "${AWS_CONNECTION_TIMEOUT?:'1000000'}"
    AWS_MAX_CONNECTIONS = "${AWS_MAX_CONNECTIONS?:'20'}"
    AWS_MAX_PARALLEL_TRANSFERS = "${AWS_MAX_PARALLEL_TRANSFERS?:'10'}"

    // aws batch config
    QUEUE = "${QUEUE}"

    // k8s config
    K8S_NODE = "${K8S_NODE}"
    K8S_PULL_POLICY = "${K8S_PULL_POLICY?:'Always'}"
    K8S_SERVICE_ACCOUNT = "${K8S_SERVICE_ACCOUNT}"
    K8S_STORAGE_CLAIM_NAME = "${K8S_STORAGE_CLAIM_NAME}"
    K8S_STORAGE_MOUNT_PATH = "${K8S_STORAGE_MOUNT_PATH}"

    // Process
    QUEUE_SIZE = "${QUEUE_SIZE}"
    PROCESS_MAX_RETRIES = "${PROCESS_MAX_RETRIES?:'3'}"
    PROCESS_CPU_LOW = "${PROCESS_CPU_LOW?:'1'}"
    PROCESS_CPU_HIGH = "${PROCESS_CPU_HIGH?:'2'}"
    PROCESS_MEMORY_VERY_LOW = "${PROCESS_MEMORY_VERY_LOW?:'8000'}"
    PROCESS_MEMORY_LOW = "${PROCESS_MEMORY_LOW?:'8000'}"
    PROCESS_MEMORY_MEDIUM = "${PROCESS_MEMORY_MEDIUM?:'8000'}"
    PROCESS_MEMORY_HIGH = "${PROCESS_MEMORY_HIGH?:'6000'}"
    PROCESS_MEMORY_VERY_HIGH = "${PROCESS_MEMORY_VERY_HIGH?:'16000'}"

    SARS_COV_2_PIPELINE_DOCKER_IMAGE = "${SARS_COV_2_PIPELINE_DOCKER_IMAGE?:'sars-cov-2-pipeline'}"
    NCOV_DOCKER_IMAGE_NAME = "ncov2019-artic-nf-${params.sequencing_technology}"

}

params {
    // TODO: Handle this MUCH better
    contamination_removal_empty_csv = "/app/scripts/contamination_removal_empty.csv"
    primer_autodetection_empty_csv = "/app/scripts/primer_autodetection_empty.csv"
    ncov_qc_empty_csv = "/app/scripts/ncov_qc_empty.csv"
    ncov_typing_empty_csv = "/app/scripts/ncov_typing_empty.csv"
    pangolin_empty_csv = "/app/scripts/pangolin_empty.csv"
}

aws {
    client {
        maxConnections = "${env.AWS_MAX_CONNECTIONS}"
        connectionTimeout = "${env.AWS_CONNECTION_TIMEOUT}"
        maxErrorRetry = 5
    }
    batch {
        maxParallelTransfers = "${env.AWS_MAX_PARALLEL_TRANSFERS}"
        maxTransferAttempts = 5
        volumes = ["/app/reference:/app/reference:ro"]
    }
}

k8s {
   // use k8s jobs instead of pods. See: https://github.com/nextflow-io/nextflow/pull/2751
   computeResourceType = 'Job'

   pullPolicy = "${env.K8S_PULL_POLICY}"
   serviceAccount = "${env.K8S_SERVICE_ACCOUNT}"
   storageClaimName = "${env.K8S_STORAGE_CLAIM_NAME}"
   storageMountPath = "${env.K8S_STORAGE_MOUNT_PATH}"
   storageSubPath = ''
}

executor {
    queueSize = "${env.QUEUE_SIZE}"
}

process {
    cpus = "${env.PROCESS_CPU_LOW}"
    memory = { "${env.PROCESS_MEMORY_LOW}" as int * 1.MB * task.attempt }
    maxRetries = "${env.PROCESS_MAX_RETRIES}"
    // by default, re-run a process if exit status is not 0 for max retries.
    // if the process still fails, terminate the pipeline
    errorStrategy = { (task.exitStatus != 0 && task.attempt<= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'terminate' }

    container = "${env.DOCKER_IMAGE_URI_PATH}/${env.SARS_COV_2_PIPELINE_DOCKER_IMAGE}:${env.DOCKER_IMAGE_TAG}"
    queue = "${env.QUEUE}"

    withName:NCOV2019_ARTIC_NF_PIPELINE {
        container = "${env.DOCKER_IMAGE_URI_PATH}/${env.NCOV_DOCKER_IMAGE_NAME}:${env.DOCKER_IMAGE_TAG}"
        memory = { "${env.PROCESS_MEMORY_VERY_HIGH}" as int * 1.MB * task.attempt }
        cpus = "${env.PROCESS_CPU_HIGH}" // TODO: only for illumina
        // if the computation for 1 sample dies, do not interrupt the computation for the other samples
        errorStrategy = { (task.exitStatus != 0 && task.attempt<= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'ignore' }
    }

    withName:PANGOLIN_PIPELINE {
        container = "${env.DOCKER_IMAGE_URI_PATH}/pangolin:${env.DOCKER_IMAGE_TAG}"
        memory = { "${env.PROCESS_MEMORY_HIGH}" as int * 1.MB * task.attempt }
        // if the computation for 1 sample dies, do not interrupt the computation for the other samples
        errorStrategy = { (task.exitStatus != 0 && task.attempt    <= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'ignore' }
    }
}


// enable these passing flags like: -with-trace
trace {
  enabled = false
  file = "${params.run}_trace.csv"
  sep = ','
  raw = true  // time: ms; memory: bytes
  overwrite = true
  //default:
  //fields = 'task_id,hash,native_id,name,status,exit,submit,duration,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar'
  fields = 'task_id,hash,native_id,name,status,exit,submit,start,complete,duration,memory,attempt,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar,error_action'
}

//send trace scope information as HTTP POST request to a webserver, shipped as a JSON object
weblog {
  enabled = false
  url = 'http://localhost'
}

report {
  enabled = false
  file = "${params.run}_report.html"
  overwrite = true
}

timeline {
  enabled = false
  file = "${params.run}_timeline.html"
  overwrite = true
}

dag {
  enabled = false
  file = "${params.run}_dag.svg"
  overwrite = true
}
