env {
    PSGA_ROOT_PATH = "/app"
    // store temporary files in the scratch directory instead of nextflow usual work directory
    // use false only for testing
    SCRATCH = "${SCRATCH?:'true'}"

    // docker image config
    DOCKER_IMAGE_URI_PATH = "${DOCKER_IMAGE_URI_PATH}"
    DOCKER_IMAGE_TAG = "${DOCKER_IMAGE_TAG}"

    // aws config
    AWS_CONNECTION_TIMEOUT = "${AWS_CONNECTION_TIMEOUT?:'1000000'}"
    AWS_MAX_CONNECTIONS = "${AWS_MAX_CONNECTIONS?:'20'}"
    AWS_MAX_PARALLEL_TRANSFERS = "${AWS_MAX_PARALLEL_TRANSFERS?:'10'}"

    // aws batch config
    QUEUE = "${QUEUE}"

    // k8s config
    K8S_NODE = "${K8S_NODE}"
    K8S_PULL_POLICY = "${K8S_PULL_POLICY?:'Always'}"
    K8S_SERVICE_ACCOUNT = "${K8S_SERVICE_ACCOUNT}"
    K8S_STORAGE_CLAIM_NAME = "${K8S_STORAGE_CLAIM_NAME}"
    K8S_STORAGE_MOUNT_PATH = "${K8S_STORAGE_MOUNT_PATH}"

    // process
    QUEUE_SIZE = "${QUEUE_SIZE}"
    PROCESS_MAX_RETRIES = "${PROCESS_MAX_RETRIES?:'3'}"
    PROCESS_CPU_LOW = "${PROCESS_CPU_LOW?:'1'}"
    PROCESS_CPU_HIGH = "${PROCESS_CPU_HIGH?:'2'}"
    PROCESS_MEMORY_VERY_LOW = "${PROCESS_MEMORY_VERY_LOW?:'8000'}"
    PROCESS_MEMORY_LOW = "${PROCESS_MEMORY_LOW?:'8000'}"
    PROCESS_MEMORY_MEDIUM = "${PROCESS_MEMORY_MEDIUM?:'8000'}"
    PROCESS_MEMORY_HIGH = "${PROCESS_MEMORY_HIGH?:'8000'}"
    PROCESS_MEMORY_VERY_HIGH = "${PROCESS_MEMORY_VERY_HIGH?:'16000'}"
}

params {
    // analysis run name
    run = ""
    // path to the metadata CSV file
    metadata = ""
    // e.g. 'illumina', 'ont', 'unknown'
    sequencing_technology = ""
    // the kit used for sequencing the sample (e.g. 'V4' as primer-scheme version)
    kit = ""
    // the path to the pipeline output. This can be s3
    output_path = ""

    /* Optional parameters */
    help = false
    print_config = false
}


aws {
    client {
        maxConnections = "${env.AWS_MAX_CONNECTIONS}"
        connectionTimeout = "${env.AWS_CONNECTION_TIMEOUT}"
        maxErrorRetry = 5
    }
    batch {
        maxParallelTransfers = "${env.AWS_MAX_PARALLEL_TRANSFERS}"
        maxTransferAttempts = 5
        volumes = ["/app/reference:/app/reference:ro"]
    }
}

k8s {
   // use k8s jobs instead of pods. See: https://github.com/nextflow-io/nextflow/pull/2751
   computeResourceType = 'Job'

   pullPolicy = "${env.K8S_PULL_POLICY}"
   serviceAccount = "${env.K8S_SERVICE_ACCOUNT}"
   storageClaimName = "${env.K8S_STORAGE_CLAIM_NAME}"
   storageMountPath = "${env.K8S_STORAGE_MOUNT_PATH}"
   storageSubPath = ''
}

executor {
    queueSize = "${env.QUEUE_SIZE}"
}

process {

    // default directives. These can be fine-tuned per process
    cpus = "${env.PROCESS_CPU_LOW}"
    memory = { "${env.PROCESS_MEMORY_LOW}" as int * 1.MB * task.attempt }
    maxRetries = "${env.PROCESS_MAX_RETRIES}"
    // by default, re-run a process if exit status is not 0 for max retries.
    // if the process still fails, terminate the pipeline
    errorStrategy = { (task.exitStatus != 0 && task.attempt <= "${env.PROCESS_MAX_RETRIES}" as int) ? 'retry' : 'terminate' }

    queue = "${env.QUEUE}"

    // The scratch directive allows you to execute the process in a temporary folder that is local to the execution node.
    // This is useful when your pipeline is launched by using a grid executor, because it allows you to decrease the NFS
    // overhead by running the pipeline processes in a temporary directory in the local disk of the actual execution node.
    // Only the files declared as output in the process definition will be copied in the pipeline working area.
    scratch = "${env.SCRATCH}"

    // run the pod in the specified node affinity
    // Note the extra array brackets because matchExpressions is inside an array element of nodeSelectorTerms.
    pod = [
      affinity: [
        nodeAffinity: [
          requiredDuringSchedulingIgnoredDuringExecution: [
            nodeSelectorTerms: [[
              matchExpressions: [
                [key: "${env.K8S_NODE}", operator: "In", values: ["true"]]
              ]
            ]]
          ]
        ]
      ]
    ]
}

// NOTE: these are not uploaded to s3. Looks like this is a bug in Nextflow as I've seen documentation for this use case

// enable these passing flags like: -with-trace
trace {
  enabled = false
  file = "${params.run}_trace.csv"
  sep = ','
  raw = true  // time: ms; memory: bytes
  overwrite = true
  //default:
  //fields = 'task_id,hash,native_id,name,status,exit,submit,duration,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar'
  fields = 'task_id,hash,native_id,name,status,exit,submit,start,complete,duration,memory,attempt,realtime,%cpu,peak_rss,peak_vmem,rchar,wchar,error_action'
}

//send trace scope information as HTTP POST request to a webserver, shipped as a JSON object
weblog {
  enabled = false
  url = 'http://localhost'
}

report {
  enabled = false
  file = "${params.run}_report.html"
  overwrite = true
}

timeline {
  enabled = false
  file = "${params.run}_timeline.html"
  overwrite = true
}

dag {
  enabled = false
  file = "${params.run}_dag.svg"
  overwrite = true
}
